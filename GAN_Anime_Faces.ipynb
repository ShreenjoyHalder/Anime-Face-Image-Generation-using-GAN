{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPanCAQsRCMuqjpIqe9dI++"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- Installing or upgrading the opendatasets Python package to install dataset from kaggle."],"metadata":{"id":"sTdfVpUZmQip"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxTBMB-z3R3i"},"outputs":[],"source":["!pip install opendatasets --upgrade --quiet"]},{"cell_type":"code","source":["import opendatasets as od"],"metadata":{"id":"AfTvdjKqKie3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Since it is kaggle dataset you will a kaggle API token to download it."],"metadata":{"id":"rYxLSBX4kVzb"}},{"cell_type":"code","source":["dataset_url='https://www.kaggle.com/splcher/animefacedataset'\n","od.download(dataset_url)"],"metadata":{"id":"BeH7V6mkK3fD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Prints a list of files/folders inside the directory `./animefacedataset`."],"metadata":{"id":"oNDlpLTnWcRq"}},{"cell_type":"code","source":["import os\n","\n","data_dir=\"./animefacedataset\"\n","print(os.listdir(data_dir))"],"metadata":{"id":"t932EH5XL54V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Let us load this dataset using the `ImageFolder` class from `torchvision`. We will also resize and crop the\n","images to `64px * 64px`, and normalize the pixel values with a mean & standard deviation of `0.5` for each channel.\n","This will ensure that pixel values are in the range `(-1, 1)`."],"metadata":{"id":"VLLvOxwtNCGG"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torchvision.transforms as T"],"metadata":{"id":"oI6ZNL5BE9t7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_size=64\n","batch_size=64\n","stats=((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))"],"metadata":{"id":"6ncCG5L32Qyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Implementing an image preprocessing pipeline."],"metadata":{"id":"na8s72YbXA7G"}},{"cell_type":"code","source":["tf=T.Compose([T.Resize(image_size),\n","             T.CenterCrop(image_size),\n","             T.ToTensor(),\n","             T.Normalize(*stats)])"],"metadata":{"id":"Cb57o7G7EunV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_ds=ImageFolder(data_dir, transform=tf)\n","training_dl=DataLoader(training_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)"],"metadata":{"id":"3hC_LAcgHgZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"AHIV9lCJMn5-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To reverse the normalization applied during preprocessing for visualization."],"metadata":{"id":"1B5_CMKxX0_D"}},{"cell_type":"code","source":["def denorm(img_tensors):\n","  return img_tensors*stats[1][0]+stats[0][0]"],"metadata":{"id":"UqakMSceMy-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To visualize a batch of images."],"metadata":{"id":"JKHOjq03YA2B"}},{"cell_type":"code","source":["def show_batch(dl):\n","  for i, _ in dl:\n","    show_image(i)\n","    break\n","\n","def show_image(i):\n","  fig, ax=plt.subplots(figsize=(8, 8))\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  denorm_i=denorm(i)\n","  ax.imshow(make_grid(denorm_i, nrow=8).permute(1, 2, 0).clamp(0, 1))"],"metadata":{"id":"c9hUJMmMQfJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_batch(training_dl)"],"metadata":{"id":"rR5vc_2nQxzr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Changing runtime to GPU"],"metadata":{"id":"FfVPAvYrR1s-"}},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"kl3a5RVWQ7f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def default_device():\n","  if torch.cuda.is_available():\n","    return torch.device(\"cuda\")\n","  return torch.device(\"cpu\")"],"metadata":{"id":"rk7JIbF8RmCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device=default_device()\n","device"],"metadata":{"id":"eGx5Lie0Rn09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_device(data, device):\n","  if isinstance(data, (list, tuple)):\n","    return [to_device(x, device) for x in data]\n","  return data.to(device, non_blocking=True)"],"metadata":{"id":"Dozg7evTRptM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, _ in training_dl:\n","  print(i.shape)\n","  print(i.device)\n","  i=to_device(i, device)\n","  print(i.device)\n","  break"],"metadata":{"id":"xK5Zp3g6Rr0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class deviceDataLoader():\n","\n","  def __init__(self, dl, device):\n","    self.dl=dl\n","    self.device=device\n","\n","  def __iter__(self):\n","    for i in self.dl:\n","      yield to_device(i, self.device)\n","\n","  def __len__(self):\n","    return len(self.dl)"],"metadata":{"id":"3tXqBz4mRtw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device=default_device()"],"metadata":{"id":"Qajr9qMsRwGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Loading the data loader to GPU (if available)."],"metadata":{"id":"usBdI6cxYSb4"}},{"cell_type":"code","source":["training_dl=deviceDataLoader(training_dl, device)"],"metadata":{"id":"V4GvPUtARx3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Discriminator network"],"metadata":{"id":"vELBH4ICS3Yv"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"rnQoaxetSvm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator=nn.Sequential(\n","    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(64),\n","    nn.LeakyReLU(0.2, inplace=True),\n","\n","    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(128),\n","    nn.LeakyReLU(0.2, inplace=True),\n","\n","    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(256),\n","    nn.LeakyReLU(0.2, inplace=True),\n","\n","    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(512),\n","    nn.LeakyReLU(0.2, inplace=True),\n","\n","    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n","\n","    nn.Flatten(),\n","    nn.Sigmoid()\n",")"],"metadata":{"id":"-h147B6fTJuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["discriminator=to_device(discriminator, device)"],"metadata":{"id":"fdMqH6-vW7_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["latent_size=64"],"metadata":{"id":"tgj5x0YLHqB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generator network"],"metadata":{"id":"XgIRaQpYZNop"}},{"cell_type":"code","source":["generator=nn.Sequential(\n","    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n","    nn.BatchNorm2d(512),\n","    nn.ReLU(True),\n","\n","    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(256),\n","    nn.ReLU(True),\n","\n","    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(128),\n","    nn.ReLU(True),\n","\n","    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.BatchNorm2d(64),\n","    nn.ReLU(True),\n","\n","    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n","    nn.Tanh()\n",")"],"metadata":{"id":"oNgCd5fpHuZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xb=torch.randn(batch_size, latent_size, 1, 1)\n","print(xb.shape)\n","fake_image=generator(xb)\n","print(fake_image.shape)\n","show_image(fake_image)"],"metadata":{"id":"VfrOzUCJJJpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator=to_device(generator, device)"],"metadata":{"id":"pAAvG9sZMt83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"efjl7188cBAZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `train_d` trains the discriminator to:\n","- Output 1 for real images and 0 for fake images from generator.\n","- Compute total loss and update discriminator parameters."],"metadata":{"id":"6_nHonbJZb_u"}},{"cell_type":"code","source":["def train_d(real_images, opt_d):\n","\n","  opt_d.zero_grad()\n","\n","  real_predictions=discriminator(real_images)\n","  real_tragets=torch.ones(real_images.size(0), 1, device=device)\n","  real_loss=F.binary_cross_entropy(real_predictions, real_tragets)\n","  real_score=torch.mean(real_predictions).item()\n","\n","  latent=torch.randn(batch_size, latent_size, 1, 1, device=device)\n","  fake_images=generator(latent)\n","\n","  fake_predictions=discriminator(fake_images)\n","  fake_tragets=torch.zeros(fake_images.size(0), 1, device=device)\n","  fake_loss=F.binary_cross_entropy(fake_predictions, fake_tragets)\n","  fake_score=torch.mean(fake_predictions).item()\n","\n","  loss=real_loss+fake_loss\n","  loss.backward()\n","  opt_d.step()\n","\n","  return loss.item(), real_score, fake_score"],"metadata":{"id":"kTOkbfXPX8SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `train_g` trains the generator to:\n","- Generate fake images that can fool the discriminator.\n","- Maximize the discriminator's output.\n","- Compute loss against target label 1 and update generator parameters accordingly."],"metadata":{"id":"BocBIJrjaVXk"}},{"cell_type":"code","source":["def train_g(opt_g):\n","\n","  opt_g.zero_grad()\n","\n","  latent=torch.randn(batch_size, latent_size, 1, 1, device=device)\n","  fake_images=generator(latent)\n","\n","  predictions=discriminator(fake_images)\n","  targets=torch.ones(batch_size, 1, device=device)\n","  loss=F.binary_cross_entropy(predictions, targets)\n","\n","  loss.backward()\n","  opt_g.step()\n","\n","  return loss.item()"],"metadata":{"id":"Mv3NWhBJnDni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.utils import save_image"],"metadata":{"id":"GHtnTphbnEcR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Let us create a directory to save the generated images."],"metadata":{"id":"jXdRalh0bg1m"}},{"cell_type":"code","source":["save_dir='gen'\n","os.makedirs(save_dir, exist_ok=True)"],"metadata":{"id":"UUa2YffTnkVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- The function, save_sample, is designed to generate, save, and optionally display a grid of images generated."],"metadata":{"id":"HYWLUEsNbPkd"}},{"cell_type":"code","source":["def save_sample(idx, latent_tensor, show=True):\n","  fake_image=generator(latent_tensor)\n","  fake_fname=\"generated_img_{}.png\".format(idx)\n","  save_image(denorm(fake_image), os.path.join(save_dir, fake_fname), nrow=8)\n","  print(\"Saving\", fake_fname)\n","  if show:\n","    fig, ax=plt.subplots(figsize=(8, 8))\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    ax.imshow(make_grid(denorm(fake_image).cpu().detach(), nrow=8).permute(1, 2, 0))"],"metadata":{"id":"POfIS_9on3Cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fixed_latent=torch.randn(64, latent_size, 1, 1, device=device)"],"metadata":{"id":"5gsrwhO-tY84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_sample(0, fixed_latent)"],"metadata":{"id":"3rS1IxVwutyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"vkebR3dD4ghg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- To train the model for a given number of epochs, track losses and scores, and save generated image samples at each epoch."],"metadata":{"id":"RQL19XXeb57-"}},{"cell_type":"code","source":["def fit(epoch, lr, start_idx=1):\n","\n","  torch.cuda.empty_cache()\n","\n","  losses_g=[]\n","  losses_d=[]\n","  real_scores=[]\n","  fake_scores=[]\n","\n","  opt_d=torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n","  opt_g=torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n","\n","  for i in range(epoch):\n","    for real_i, _ in tqdm(training_dl):\n","\n","      loss_d, real_score, fake_score=train_d(real_i, opt_d)\n","      loss_g=train_g(opt_g)\n","\n","    losses_g.append(loss_g)\n","    losses_d.append(loss_d)\n","    real_scores.append(real_score)\n","    fake_scores.append(fake_score)\n","\n","    print(f\"Epoch : {i+1}, loss_g : {loss_g:.3f}, loss_d : {loss_d:.3f}, real_score : {real_score:.3f}, fake_score : {fake_score:.3f}\")\n","\n","    save_sample(i+start_idx, fixed_latent, show=False)\n","\n","  return losses_g, losses_d, real_scores, fake_scores"],"metadata":{"id":"L5inr3O7u7uC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Start the training"],"metadata":{"id":"9KUIHJV_lL3R"}},{"cell_type":"code","source":["lr=0.0002\n","epoch=10\n","history=[]"],"metadata":{"id":"mFLNYg-j20gP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history+=fit(epoch, lr)"],"metadata":{"id":"QRV3IB0d3O1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history+=fit(epoch, lr, 11)"],"metadata":{"id":"EWmGB-LvEChp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history+=fit(epoch, lr/2, 21)"],"metadata":{"id":"hm7ETkUoEOUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history+=fit(epoch, lr/2, 31)"],"metadata":{"id":"9rc6v5XNDHpx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Make a video of the saved images to view the changes throughout the epochs."],"metadata":{"id":"UP0WuUZplZD2"}},{"cell_type":"code","source":["import cv2\n","import os\n","\n","sample_dir=\"gen\"\n","files=[os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if f.endswith('.png')]\n","files.sort()\n","\n","out=cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'MP4V'), 1, (640, 640))\n","\n","for i in files:\n","  img=cv2.imread(i)\n","  out.write(cv2.resize(img, (640, 640)))\n","\n","out.release()"],"metadata":{"id":"rnBILMMyDIQT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extract generator and discriminator losses from alternating positions. Then plots both loss curves across training epochs."],"metadata":{"id":"-79vjEOAlxZB"}},{"cell_type":"code","source":["losses_g=[]\n","losses_d=[]\n","for i in range(0, len(history), 4):\n","  losses_g.extend(history[i])\n","  losses_d.extend(history[i+1])\n","plt.plot(losses_g)\n","plt.plot(losses_d)\n","plt.xlabel(\"Number of epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Generator\", \"Discriminator\"])\n","plt.show()"],"metadata":{"id":"NPlsDvaluLvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ykZqRjJNLn-"},"execution_count":null,"outputs":[]}]}